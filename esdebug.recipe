# -*- coding: utf-8 -*-
__license__   = 'GPL v3'
__copyright__ = '2010, Zsolt Botykai <zsoltika@gmail.com>'
'''
A recipe for Calibre to fetch http://www.es.hu , and generate an article list 
to fetch, then get rid of the unnecessary scrap at the site (e.g. facebook 
buttons, ads...)
'''

# The recipe modifies the case of titles and searches via regexs
import  datetime
import  os
import  string, re
from    string import capwords

from    calibre.web.feeds.news       import BasicNewsRecipe
from    calibre.ebooks.BeautifulSoup import Tag, NavigableString

class EletEsIrodalom(BasicNewsRecipe):
    title                   = u'Élet és Irodalom '
    __author__              = u'Zsolt Botykai '
    description             = u'Élet és Irodalom hetilap '
    INDEX                   = 'http://www.es.hu/'
    language                = 'hu'
    simultaneous_downloads  = 2
    timefmt                 = ' [%Y-%m-%d %H:%M]'
    tags                    = u'politika, irodalom, hetilap, újság, magyar nyelvű, vers, vélemény, kritika, interjú'
    publication_type        = 'magazine'
    remove_javascript       = True
    remove_empty_feeds      = True
    no_stylesheets          = True
    auto_cleanup            = False
    auto_cleanup_keep       = '//span[@itemprop="author"]|//h1[@itemprop="name"]'
    encoding = 'UTF-8'

    # without the background color setup, the conversion to pdf produced 
    # black pages with white text 
    extra_css = '''
                    body { background-color: white; color: black; }
                    p { text-align: justify; margin_top: 0px; margin-bottom: 0px; }
                '''

    masthead_url='https://c1.staticflickr.com/1/475/32050957890_e7d913da49_o.jpg'

    def preprocess_html(self, soup):
        for div in soup.find_all("div", class_="banner"):
            div.decompose()  # type: ignore
        for div in soup.find_all("div", class_="row"):
            div.decompose()  # type: ignore
        for div in soup.find_all("div", attrs={"id": "header"}):
            div.decompose()  # type: ignore
        for div in soup.find_all("div", attrs={"id": "sidebar"}):
            div.decompose()  # type: ignore
        for div in soup.find_all("div", attrs={"id": "footer"}):
            div.decompose()  # type: ignore
        for div in soup.find_all("div", attrs={"id": "copy"}):
            div.decompose()  # type: ignore
        for ul in soup.find_all("ul", attrs={"id": "tools"}):
            ul.decompose()  # type: ignore
        for span in soup.find_all("span", class_="lapszam"):
            span.decompose()  # type: ignore

        return soup

    def postprocess_html(self, soup, first):
        html_title     = soup.find('title').string
        if not html_title:
            # self.log(soup)
            html_title = self.tag_to_string(soup.find('title'))

        new_html_title = html_title.replace(u" | ÉLET ÉS IRODALOM","")
        self.log('PPH: title3: ', new_html_title)
        # new_title_tag  = Tag(soup, 'title')
        soup.title.clear()
        soup.title.insert(0,new_html_title)

        self.log('PPH: title4: ', new_html_title)
        #new_title_tag.insert(0,new_html_title)
        #soup.find('title').replaceWith(new_title_tag)

        artau = get_author(self,soup)

        try:
            arttit = soup.find('h1').contents[0]
        except AttributeError:
            arttit = ''
            # self.log('SOUP: ',soup)

        new_h1 = soup.new_tag('h1')

        new_h1.insert(0,artau + ': ' + arttit)
        soup.find('h1').replaceWith(new_h1)

        try:
            deltag = soup.find('span', attrs={'itemprop':'author'})
            _      = deltag.extract()
        except AttributeError:
            pass

        for para in soup.findAll('p'):
            para['height'] = 0

        return soup


    def get_browser(self):
        siteuser = os.environ['ES_USER']
        sitepass = os.environ['ES_PASS']

        br = BasicNewsRecipe.get_browser(self)
        br.open('http://www.es.hu/belepes')

        #LOGIN form filling
        br.select_form(nr=1)
        br.form.set_value(siteuser,nr=1)
        br.form.set_value(sitepass,nr=2)

        br.submit()

        return br

    def get_cover_url(self):
        return 'https://c1.staticflickr.com/1/475/32050957890_e7d913da49_o.jpg'

    def parse_index(self):
        articles = []

        soup = self.index_to_soup(self.INDEX)
        # self.log("got soup: ", soup)
        datediv = soup.find('div', attrs={'id':'header'})
        # self.log("found datediv: ", datediv)
        datespan = datediv.find('a')
        # self.log("found date: ", datespan)
        gotdate = self.tag_to_string(datespan)
        # self.log("found date: ", gotdate)

        cover = soup.find('img', src=True, attrs={'class':'cover'})
        if cover is not None:
            self.cover_url = cover['src']

        issue = re.sub(u'. évfolyam, ', '/', gotdate)
        issue = re.sub(r'\..*', '.', issue)
        # self.log("found issue: ", issue)
        # self.log("title: ", self.title)
        self.title += issue
        # self.log("title new: ", self.title)
        self.description += issue
        feeds = []

        # newspaper sections are rarely changing so it's much easier this way
        sections = [
            [u'Publicisztika'   , 'http://www.es.hu/rovat/publicisztika'] ,
            [u'Feuilleton'      , 'http://www.es.hu/rovat/feuilleton'] ,
            [u'Interjú'         , 'http://www.es.hu/rovat/interju'] ,
            [u'Visszhang'       , 'http://www.es.hu/rovat/visszhang'] ,
            [u'Páratlan oldal'  , 'http://www.es.hu/rovat/paratlan-oldal'] ,
            [u'Próza'           , 'http://www.es.hu/rovat/proza'] ,
            [u'Vers'            , 'http://www.es.hu/rovat/vers'] ,
            [u'Könyvkritika'    , 'http://www.es.hu/rovat/konyvkritika'] ,
            [u'Művészetkritika' , 'http://www.es.hu/rovat/muveszetkritika'] 
        ]

        for section in sections:
            section_title = section[0]
            sect_url      = section[1]
            articles = []
            # self.log('Processing section: ', section)
            sectsoup = self.index_to_soup(sect_url)

            # self.log('  ... check if redirected ...')
            check_if_redirected(self, sectsoup, gotdate, section, articles)

            if len(articles) > 0:
                feeds.append((section_title, articles))
                continue

            get_sect_articles(self, sectsoup, articles,gotdate)

            has_sp=sectsoup.findAll('div', attrs={'id':'pager'})
            # self.log("Lapozó: ", has_sp)
	        # self.log(section_title, '###', articles)

            if has_sp and len(articles) > 0:
                for sp in sectsoup.findAll('div', attrs={'id':'pager'}):
                    for ps in sp.findAll('a', attrs={'class':'pagerbutton'}):
                        ps_text=self.tag_to_string(ps)
                        # self.log(ps_text)
                        if re.match(r'\d+$',ps_text) and ps['href']:
                            nl='http://www.es.hu' + ps['href']
                            subpg = self.index_to_soup(nl)
                            get_sect_articles(self,subpg,articles,gotdate)

	    if len(articles) > 0:
	        feeds.append((section_title, articles))

        return feeds

def check_if_redirected(self, sectsoup, gotdate, section, articles):
    arturl  = ""
    arttit  = ""
    artau   = ""
    artdesc = ""

    isredirected = sectsoup.find('meta', attrs={'itemprop':'url'})
    if isredirected:
        # self.log('  Redirection occurred for ', section[1])
        arturl = isredirected['content']
        self.log("    IN REDIRECTED")
        self.log("    url: ", arturl)

        artau = get_author(self,sectsoup)

        # self.log("    author: ", artau)
        arttit = sectsoup.find('h1').contents[0]
        # self.log("    title: ", arttit)
        if artau and arttit and arturl :
            articles.append({'title':artau + ' - ' + arttit, 'url':arturl, 'description':artdesc,'date': gotdate})

def get_sect_articles(self, sectsoup, articles,gotdate):
    arturl  = ""
    arttit  = ""
    artau   = ""
    artdesc = ""

    for div in sectsoup.findAll('div', attrs={'itemtype':'http://schema.org/Article'}):
        # self.log('  Found article... ')
        artau = get_author(self,div)
        # self.log("    author: ", artau)
        arttit = div.h2.a.contents[0] 
        # self.log("    title: ", arttit)
        arturl = "http://www.es.hu" + div.h2.a['href']
        self.log("    IN GET_SECT_ARTICLES")
        self.log("    url: ", arturl)
        if "brigadnaplo" in arturl:
            continue

        try:
            artdesc = "".join(str(item) for item in div.div)
        except TypeError:
            artdesc = ""
        # self.log("    desc: ", artdesc)

        if artau and arttit and arturl:
            articles.append({'title':artau + ' - ' + arttit, 'url':arturl, 'description':artdesc,'date': gotdate})

        art_au  = ""
        arttit  = ""
        arturl  = ""
        artdesc = ""

def get_author(self,soup):
    artau = ''
    try:
        artau = self.tag_to_string(soup.find('span', attrs={'itemprop':'author'}))
    except AttributeError:
        artau = ''
        artau = self.tag_to_string(soup.find('span', attrs={'itemprop':'author'}))
    if not artau:
        artau = 'A szerk.'
    return artau

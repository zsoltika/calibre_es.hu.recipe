# -*- coding: utf-8 -*-
__license__   = 'GPL v3'
__copyright__ = '2010, Zsolt Botykai <zsoltika@gmail.com>'
'''
A recipe for Calibre to fetch http://www.es.hu , and generate an article list 
to fetch, then get rid of the unnecessary scrap at the site (e.g. facebook 
buttons, ads...)
'''

# The recipe modifies the case of titles and searches via regexs
import  datetime
import  os
import  string, re
from    string import capwords

from    calibre.web.feeds.news       import BasicNewsRecipe
from    calibre.ebooks.BeautifulSoup import Tag, NavigableString

class EletEsIrodalom(BasicNewsRecipe):
    title                   = u'Élet és Irodalom '
    __author__              = u'Zsolt Botykai '
    description             = u'Élet és Irodalom hetilap '
    INDEX                   = 'http://www.es.hu/'
    language                = 'hu'
    simultaneous_downloads  = 2
    timefmt                 = ' [%Y-%m-%d %H:%M]'
    tags                    = u'politika, irodalom, hetilap, újság, magyar nyelvű, vers, vélemény, kritika, interjú'
    publication_type        = 'magazine'
    remove_javascript       = True
    remove_empty_feeds      = True
    no_stylesheets          = True
    auto_cleanup            = True
    encoding = 'UTF-8'

    # without the background color setup, the conversion to pdf produced 
    # black pages with white text 
    extra_css = '''
                    body { background-color: white; color: black; }
                    p { text-align: justify; margin_top: 0px; margin-bottom: 0px; }
                '''

    masthead_url='https://c1.staticflickr.com/1/475/32050957890_e7d913da49_o.jpg'

    def postprocess_html(self, soup, first):
        html_title=soup.find('title').string
        new_html_title=html_title.replace(u" | ÉLET ÉS IRODALOM","")
        new_title_tag=Tag(soup, 'title')
        new_title_tag.insert(0,new_html_title)
        soup.find('title').replaceWith(new_title_tag)
        h2_title=soup.find('h2').string
        new_h2_title=h2_title.replace(u" | ÉLET ÉS IRODALOM","")
        new_h2=Tag(soup, 'h2')
        new_h2.insert(0,new_h2_title)
        soup.find('h2').replaceWith(new_h2)
        for para in soup.findAll('p'):
            para['height']=1

        return soup


    def get_browser(self):
        siteuser = os.environ['ES_USER']
        sitepass = os.environ['ES_PASS']

        br = BasicNewsRecipe.get_browser(self)
        br.open('http://www.es.hu/belepes')

        #LOGIN form filling
        br.select_form(nr=1)
        br.form.set_value(siteuser,nr=1)
        br.form.set_value(sitepass,nr=2)

        br.submit()

        return br

    def get_cover_url(self):
        return 'https://c1.staticflickr.com/1/475/32050957890_e7d913da49_o.jpg'

    def parse_index(self):
        articles = []

        soup = self.index_to_soup(self.INDEX)
        datediv = soup.find('div', attrs={'id':'header'})
        datespan = datediv.find('a')
        gotdate = self.tag_to_string(datespan)

        cover = soup.find('img', src=True, attrs={'class':'cover'})
        if cover is not None:
            self.cover_url = cover['src']

        issue = re.sub(u'. évfolyam, ', '/', gotdate)
        issue = re.sub(r'\..*', '.', issue)
        self.title += issue
        self.description += issue
        feeds = []

        # newspaper sections are rarely changing so it's much easier this way
        sections = [
            [u'Publicisztika'   , 'http://www.es.hu/rovat/publicisztika'] ,
            [u'Feuilleton'      , 'http://www.es.hu/rovat/feuilleton'] ,
            [u'Interjú'         , 'http://www.es.hu/rovat/interju'] ,
            [u'Visszhang'       , 'http://www.es.hu/rovat/visszhang'] ,
            [u'Páratlan oldal'  , 'http://www.es.hu/rovat/paratlan-oldal'] ,
            [u'Irodalom'        , 'http://www.es.hu/rovat/irodalom'] ,
            [u'Könyvkritika'    , 'http://www.es.hu/rovat/konyvkritika'] ,
            [u'Művészetkritika' , 'http://www.es.hu/rovat/muveszetkritika'] 
        ]

        for section in sections:
            section_title = section[0]
            sect_url      = section[1]
            articles = []
            self.log('Processing section: ', section)
            sectsoup = self.index_to_soup(sect_url)

            self.log('  ... check if redirected ...')
            check_if_redirected(self, sectsoup, gotdate, section, articles)

            if len(articles) > 0:
                feeds.append((section_title, articles))
                continue

            get_sect_articles(self, sectsoup, articles,gotdate)

            has_sp=sectsoup.findAll('div', attrs={'id':'pager'})
            # self.log("Lapozó: ", has_sp)
	        # self.log(section_title, '###', articles)

            if has_sp and len(articles) > 0:
                for sp in sectsoup.findAll('div', attrs={'id':'pager'}):
                    for ps in sp.findAll('a', attrs={'class':'pagerbutton'}):
                        ps_text=self.tag_to_string(ps)
                        # self.log(ps_text)
                        if re.match(r'\d+$',ps_text) and ps['href']:
                            nl='http://www.es.hu' + ps['href']
                            subpg = self.index_to_soup(nl)
                            get_sect_articles(self,subpg,articles,gotdate)

	    if len(articles) > 0:
	        feeds.append((section_title, articles))

        return feeds

def check_if_redirected(self, sectsoup, gotdate, section, articles):
    arturl  = ""
    arttit  = ""
    artau   = ""
    artdesc = ""

    isredirected = sectsoup.find('meta', attrs={'itemprop':'url'})
    if isredirected:
        self.log('  Redirection occurred for ', section[1])
        arturl = isredirected['content']
        self.log("    url: ", arturl)
        artau  = sectsoup.find('span', attrs={'itemprop':'author'}).string
        self.log("    author: ", artau)
        arttit = sectsoup.find('h1').contents[0]
        self.log("    title: ", arttit)
        if artau and arttit and arturl :
            articles.append({'title':artau + ' - ' + arttit, 'url':arturl, 'description':artdesc,'date': gotdate})

def get_sect_articles(self, sectsoup, articles,gotdate):
    arturl  = ""
    arttit  = ""
    artau   = ""
    artdesc = ""

    for div in sectsoup.findAll('div', attrs={'itemtype':'http://schema.org/Article'}):
        self.log('  Found article... ')

        try:
            artau = div.span.contents[0]
        except AttributeError:
            artau = 'A szerk.'

        self.log("    author: ", artau)
        arttit = div.h2.a.contents[0]
        self.log("    title: ", arttit)
        arturl = "http://www.es.hu" + div.h2.a['href']
        self.log("    url: ", arturl)
        artdesc = "".join(str(item) for item in div.div)
        self.log("    desc: ", artdesc)

        if artau and arttit and arturl :
            articles.append({'title':artau + ' - ' + arttit, 'url':arturl, 'description':artdesc,'date': gotdate})

        art_au = ""
        arttit = ""
        arturl = ""
        artdesc = ""

